{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a191439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import kmapper as km\n",
    "import sklearn\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import signal\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from networkx.algorithms.similarity import graph_edit_distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1f2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k(spectrum, minimum_energy = 0.9):\n",
    "    running_total = 0.0 \n",
    "    \n",
    "    total = sum(spectrum)\n",
    "    if total == 0.0:\n",
    "        return len(spectrum)\n",
    "    for i in range(len(spectrum)):\n",
    "        running_total += spectrum[i]\n",
    "        if running_total / total >= minimum_energy:\n",
    "            return i + 1\n",
    "    return len(spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc853f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(graph1,graph2):\n",
    "    laplacian1 = nx.spectrum.laplacian_spectrum(graph1)\n",
    "    laplacian2 = nx.spectrum.laplacian_spectrum(graph2)\n",
    "    \n",
    "    k1 = select_k(laplacian1)\n",
    "    k2 = select_k(laplacian2)\n",
    "    k = min(k1, k2) #k are different between the two graphs, then use the smaller one.\n",
    "    similarity = sum((laplacian1[:k] - laplacian2[:k])**2) #sum of the squared differences between the largest k eigenvalues\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d39dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_graph_features(graph):\n",
    "    pr = nx.pagerank(graph,0.9)\n",
    "    dc = nx.degree_centrality(graph)\n",
    "    cc = nx.closeness_centrality(graph)\n",
    "    bx = nx.betweenness_centrality(graph)\n",
    "    c = nx.clustering(graph)\n",
    "    \n",
    "    #create list for each features\n",
    "    pr_list =  [i for i in pr.values()]\n",
    "    dc_list =  [i for i in dc.values()]\n",
    "    cc_list =  [i for i in cc.values()]\n",
    "    bx_list =  [i for i in bx.values()]\n",
    "    c_list =  [i for i in c.values()]\n",
    "    d_list = [val for (node, val) in graph.degree()]\n",
    "    data = np.column_stack((pr_list,dc_list,cc_list,bx_list,c_list,d_list))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ed3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TDA_transformation(data):\n",
    "    Xfilt = data\n",
    "    mapper = km.KeplerMapper()\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    Xfilt = scaler.fit_transform(Xfilt)\n",
    "    lens = mapper.fit_transform(Xfilt, projection=sklearn.manifold.TSNE())\n",
    "    cls = 2  # We use cls= 5\n",
    "\n",
    "    graph = mapper.map(lens,Xfilt,clusterer=sklearn.cluster.KMeans(n_clusters=cls,random_state=1618033),\n",
    "        cover=km.Cover(n_cubes=5, perc_overlap=0.1))\n",
    "    return km.to_nx(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bec9d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node_to_graph(graph,p):\n",
    "    new_node = graph.number_of_nodes() + 1\n",
    "    graph.add_node(new_node)\n",
    "    existing_nodes = list(graph.nodes())[:-1]  # Exclude the new node\n",
    "    for existing_node in existing_nodes:\n",
    "        if random.random() < p:  \n",
    "            graph.add_edge(new_node, existing_node)\n",
    "    return graph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f5379df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_least_degree_node(graph):\n",
    "    degrees = graph.degree()\n",
    "    min_degree_node = min(degrees, key=lambda x: x[1])[0]\n",
    "    # Remove the node with the minimum degree\n",
    "    graph.remove_node(min_degree_node)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a244e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_add_new_edge(graph):\n",
    "    node_num = graph.number_of_nodes()\n",
    "    node1 = random.randint(1,node_num)\n",
    "    node2 = random.randint(1,node_num)\n",
    "    while graph.has_edge(node1,node2) or node1 == node2 :\n",
    "        node1 = random.randint(1,node_num)\n",
    "        node2 = random.randint(1,node_num)\n",
    "    graph.add_edge(node1,node2)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f910fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_remove_edge(graph):\n",
    "    random_edge = random.choice(list(graph.edges()))\n",
    "    # Remove the randomly selected edge\n",
    "    graph.remove_edge(*random_edge)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21108a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_generator(original_graph, node, edge,p):\n",
    "    new_graph = original_graph.copy()\n",
    "    if node < 0:\n",
    "        for i in range(abs(node)):\n",
    "            new_graph = remove_least_degree_node(new_graph)\n",
    "    elif node > 0:\n",
    "        for i in range(abs(node)):\n",
    "            new_graph = add_node_to_graph(new_graph,p)\n",
    "    \n",
    "    if edge < 0:\n",
    "        for i in range(abs(edge)):\n",
    "            new_graph = random_remove_edge(new_graph)\n",
    "    elif edge >0:\n",
    "        for i in range(abs(edge)):\n",
    "            new_graph = random_add_new_edge(new_graph)\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd77d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TDA_average_similarity_for_hop(graph, hop,p):\n",
    "    counter = 0\n",
    "    sum = 0\n",
    "    for i in range(-hop,hop + 1):\n",
    "        for j in range(-hop,hop + 1):\n",
    "            if (i == -hop or i == hop or j == -hop or j == hop):\n",
    "                neighbour = graph_generator(graph,j,i,p)\n",
    "                TDA_graph = TDA_transformation(extract_graph_features(graph))\n",
    "                TDA_neighbour = TDA_transformation(extract_graph_features(neighbour))\n",
    "                score = calculate_similarity(TDA_graph, TDA_neighbour)\n",
    "                sum += score\n",
    "                counter += 1\n",
    "    return sum/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c357535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TDA_transformation(data):\n",
    "    Xfilt = data\n",
    "    mapper = km.KeplerMapper()\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    Xfilt = scaler.fit_transform(Xfilt)\n",
    "    lens = mapper.fit_transform(Xfilt, projection=sklearn.manifold.TSNE())\n",
    "    cls = 2  # We use cls= 5\n",
    "\n",
    "    graph = mapper.map(lens,Xfilt,clusterer=sklearn.cluster.KMeans(n_clusters=cls,random_state=1618033),\n",
    "        cover=km.Cover(n_cubes=5, perc_overlap=0.3))\n",
    "    return km.to_nx(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f2686dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_both_average_similarity_for_hop(graph, hop,p):\n",
    "    counter = 0\n",
    "    sum_original = 0\n",
    "    sum_TDA = 0\n",
    "    for i in range(-hop,hop + 1):\n",
    "        for j in range(-hop,hop + 1):\n",
    "            if (i == -hop or i == hop or j == -hop or j == hop):\n",
    "                neighbour = graph_generator(graph,j,i,p)\n",
    "                TDA_graph = TDA_transformation(extract_graph_features(graph))\n",
    "                TDA_neighbour = TDA_transformation(extract_graph_features(neighbour))\n",
    "                \n",
    "                score_TDA = calculate_similarity(TDA_graph, TDA_neighbour)\n",
    "                score_original = calculate_similarity(graph, neighbour)\n",
    "                \n",
    "                sum_TDA += score_TDA\n",
    "                sum_original += score_original\n",
    "                counter += 1\n",
    "    return {\"TDA\": sum_TDA/counter,\"original\": sum_original/counter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da811de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.erdos_renyi_graph(30,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f9145fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1913387736550756 55.133771874274025\n",
      "20.910601108106974 54.39359944250961\n",
      "44.07558497838127 58.33701843217739\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1776\\3404276517.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_both_average_similarity_for_hop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mscore_TDA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TDA\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mscore_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"original\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_original\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_TDA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1776\\711079002.py\u001b[0m in \u001b[0;36mcalc_both_average_similarity_for_hop\u001b[1;34m(graph, hop, p)\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[0mneighbour\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mTDA_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTDA_transformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_graph_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[0mTDA_neighbour\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTDA_transformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_graph_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneighbour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0mscore_TDA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTDA_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTDA_neighbour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1776\\450173263.py\u001b[0m in \u001b[0;36mTDA_transformation\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m  \u001b[1;31m# We use cls= 5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     graph = mapper.map(lens,Xfilt,clusterer=sklearn.cluster.KMeans(n_clusters=cls,random_state=1618033),\n\u001b[0m\u001b[0;32m     10\u001b[0m         cover=km.Cover(n_cubes=5, perc_overlap=0.3))\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mkm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_nx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\kmapper\\kmapper.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, lens, X, clusterer, cover, nerve, precomputed, remove_duplicate_nodes)\u001b[0m\n\u001b[0;32m    539\u001b[0m                     \u001b[0mfit_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m                 \u001b[0mcluster_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclusterer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m         \"\"\"\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m             \u001b[1;31m# run a k-means once\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m             labels, inertia, centers, n_iter_ = kmeans_single(\n\u001b[0m\u001b[0;32m   1187\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[1;34m(X, sample_weight, centers_init, max_iter, verbose, x_squared_norms, tol, n_threads)\u001b[0m\n\u001b[0;32m    537\u001b[0m         )\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m     \u001b[0minertia\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_inertia\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minertia\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1,20):\n",
    "    result = calc_both_average_similarity_for_hop(g,i,0.6)\n",
    "    score_TDA = result[\"TDA\"]\n",
    "    score_original = result[\"original\"]\n",
    "    print(score_original,score_TDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bd966ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backbone_pruning(adj_matrix, threshold):\n",
    "    pruned_matrix = adj_matrix.copy()\n",
    "    pruned_matrix[pruned_matrix < threshold] = 0\n",
    "    return pruned_matrix\n",
    "\n",
    "def original_bp_similarity(graph_a, graph_b, threshold):\n",
    "    # Convert graphs to adjacency matrices\n",
    "    adj_matrix_a = nx.adjacency_matrix(graph_a).toarray()\n",
    "    adj_matrix_b = nx.adjacency_matrix(graph_b).toarray()\n",
    "\n",
    "    # Apply backbone pruning to adjacency matrices\n",
    "    pruned_matrix_a = backbone_pruning(adj_matrix_a, threshold)\n",
    "    pruned_matrix_b = backbone_pruning(adj_matrix_b, threshold)\n",
    "\n",
    "    # Calculate similarity score based on the Jaccard index of pruned matrices\n",
    "    intersection = np.logical_and(pruned_matrix_a, pruned_matrix_b)\n",
    "    union = np.logical_or(pruned_matrix_a, pruned_matrix_b)\n",
    "    similarity_score = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8be83d87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 900 while Y.shape[1] == 961",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1776\\4146717548.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Calculate the similarity score between the two graphs using cosine similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0msimilarity_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_similarity_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Similarity Score:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarity_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1776\\4146717548.py\u001b[0m in \u001b[0;36mgraph_similarity_score\u001b[1;34m(graph_a, graph_b)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Calculate cosine similarity between the feature vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0msimilarity_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msimilarity_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1249\u001b[0m     \u001b[1;31m# to avoid recursive import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    179\u001b[0m             )\n\u001b[0;32m    180\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    182\u001b[0m             \u001b[1;34m\"Incompatible dimension for X and Y matrices: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;34m\"X.shape[1] == %d while Y.shape[1] == %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 900 while Y.shape[1] == 961"
     ]
    }
   ],
   "source": [
    "def graph_similarity_score(graph_a, graph_b):\n",
    "    # Convert graphs to adjacency matrices\n",
    "    adj_matrix_a = nx.to_numpy_array(graph_a)\n",
    "    adj_matrix_b = nx.to_numpy_array(graph_b)\n",
    "\n",
    "    # Flatten the adjacency matrices to obtain feature vectors\n",
    "    vector_a = adj_matrix_a.flatten()\n",
    "    vector_b = adj_matrix_b.flatten()\n",
    "\n",
    "    # Reshape feature vectors to be 2D arrays for cosine similarity calculation\n",
    "    vector_a = np.reshape(vector_a, (1, -1))\n",
    "    vector_b = np.reshape(vector_b, (1, -1))\n",
    "\n",
    "    # Calculate cosine similarity between the feature vectors\n",
    "    similarity_score = cosine_similarity(vector_a, vector_b)[0, 0]\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "# Example usage\n",
    "# Create two graphs using NetworkX\n",
    "graph_a = nx.erdos_renyi_graph(30,0.6)\n",
    "graph_a.add_edges_from([(1, 2), (2, 3)])\n",
    "\n",
    "graph_b = nx.erdos_renyi_graph(31,0.6)\n",
    "graph_b.add_edges_from([(1, 2), (2, 4)])\n",
    "\n",
    "# Calculate the similarity score between the two graphs using cosine similarity\n",
    "similarity_score = graph_similarity_score(graph_a, graph_b)\n",
    "print(\"Similarity Score:\", similarity_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
